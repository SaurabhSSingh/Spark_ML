{"cells":[{"cell_type":"code","source":["import warnings\nwarnings.filterwarnings('ignore')"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pandas as pd\ntrain_data = sqlContext.read.load('/FileStore/tables/train.csv',format='com.databricks.spark.csv',\n                                  header='true',\n                                  inferSchema='true')\npd.DataFrame(train_data.take(3), columns = train_data.columns)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Basic Statistics\ntrain_data.describe().toPandas().transpose()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, when, count, col\n\ntrain_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_data.columns]).show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["test_data = sqlContext.read.load('/FileStore/tables/test.csv',format='com.databricks.spark.csv',\n                                  header='true',\n                                  inferSchema='true')\npd.DataFrame(test_data.take(3), columns = test_data.columns)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["test_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in test_data.columns]).show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["avgAge = train_data.select(\"Age\").unionAll(test_data.select(\"Age\"))\nagecount = avgAge.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["train_data.count() + test_data.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import col, avg\navgAge.agg(avg(col(\"Age\"))).show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df = train_data.fillna({\"Age\" : 30})\ndf = df.fillna({\"Embarked\" : \"S\"})"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df1 = test_data.fillna({\"Age\" : 30})"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df1.columns]).show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df = train_data.unionAll(test_data)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.functions import split\nsplit_col = split(df['Name'], ',')\ndf_name = df.withColumn('Title', split_col.getItem(1))\n#df = df.withColumn('NAME2', split_col.getItem(1))\ndf_name.show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import *\nnewDf1 = df_name.withColumn('Title', regexp_replace('Title', ' ', '   '))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["newDf1.show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["split_col = split(newDf1['Title'], ' ')\nnewDf1 = newDf1.withColumn('Title', split_col.getItem(3))\nnewDf1.show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["newDf = newDf1.withColumn('Title', regexp_replace('Title', 'Miss.', 'Miss'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Mrs.', 'M_rs'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Ms.', 'Miss'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Mme.', 'Miss'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Mlle.', 'Miss'))\n#newDf = newDf.withColumn('Title', regexp_replace('Title', 'Mrs.', 'Miss'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Dr.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Major.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Lady.', 'Mrs'))\n#newDf = newDf.withColumn('Title', regexp_replace('Title', 'Mrs.', 'Miss'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Countess.', 'Mrs'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Don.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Sir.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Capt.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Col.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Rev.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Jonkheer.', 'Mr'))\nnewDf = newDf.withColumn('Title', regexp_replace('Title', 'Mr.', 'Mr'))\ndf = newDf.withColumn('Title', regexp_replace('Title', 'Master.', 'Master'))\ndf.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["df = df.withColumn('Title', regexp_replace('Title', 'the', 'M_rs'))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["X=df.select(df.Title).distinct().show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df0 = df.select(df.Pclass, df.Sex, df.Age, df.SibSp, df.Parch, df.Fare, df.Embarked, df.Title)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["df0.printSchema()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = ['Pclass','Sex','Embarked']\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"Survived\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["cols = train_data.columns\n# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(train_data)\ndataset0 = pipelineModel.transform(train_data)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset0 = dataset0.select(selectedcols)\n#display(dataset)\n#type(dataset)\ndataset0.show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["dataset0.show()"],"metadata":{},"outputs":[],"execution_count":30}],"metadata":{"name":"hw","notebookId":4279052616573643},"nbformat":4,"nbformat_minor":0}
